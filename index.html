<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }

  /* added by yc */

  padding {
    padding: 50px 0px;
  }

  </style>
  <link rel="icon" type="image/png" href="images/ucb.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Sangryul Jeon</title>
  <meta name="Sangryul Jeon's Homepage" http-equiv="Content-Type" content="Sangryul Jeon's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-64069893-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<!-- <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20"> -->
<table width="960" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Yen-Chi Cheng</font><br> -->
    <pageheading>Sangryul Jeon</pageheading><br>
    <!-- <b>email</b>: charlescheng0117_at_gmail_dot_com -->
    <!-- <b>email</b>: yenchicheng_at_cmu.edu -->
    <!-- <b>email</b>: yenchich_at_andrew.cmu.edu -->
    <!-- <b>email</b>: yenchich_at_cs.cmu.edu -->
    <b>email</b>: srjeon _at_ berkeley.edu
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <!-- <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', '1cm0asg17eccmilnrlah.oge@h',
        [14, 8, 19, 13, 20, 7, 12, 15, 16, 6, 1, 24, 26, 21, 5, 11, 4, 22, 3, 9, 23, 25, 18, 10, 17, 2]);
        // 'emailScramble', 'charlescheng0117@gmail.com',
        // [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]);
    </script> -->
  </p>

  <tr>
    <!-- <td width="32%" valign="top"><a href="images/yenchicheng.png"><img src="images/yenchicheng.png" width="100%" style="border-radius:15px"></a> -->
    <td width="30%" valign="top"><a href="bww_22aug.jpg"><img src="bww_22aug.jpg" width="100%" style="border-radius:15px"></a>
        <p align=center>
          | <a href="https://drive.google.com/file/d/1Hm9-25a-wLDyfM5hiXU7wuXfCO5SIqDT/view?usp=sharing">CV</a> |
          <a href="https://scholar.google.com/citations?user=3s4zIroAAAAJ&hl=en">Google Scholar</a> |
          <a href="https://www.linkedin.com/in/sangryul-jeon-3b697b17a/">LinkedIn</a> |
      </p>
    </td>

    <td width="66%" valign="top" align="justify">
        <p>
        I am currently a postdoctoral scholar in Electrical Engineering and Computer Science
	at <a href="https://eecs.berkeley.edu/">University of California, Berkeley</a>, working in <a href="https://www.icsi.berkeley.edu/icsi/">International Computer Science Institute</a>
	advised by <a href="https://www1.icsi.berkeley.edu/~stellayu/">Prof. Stella X. Yu.</a>
        <br>
        <br>
        I obtained my B.S., and Ph.D. degrees advised by <a href="https://diml.yonsei.ac.kr/">Prof. Kwanghoon Sohn</a>
        in School of Electrical and Electronic Engineering from <a href="https://ee.yonsei.ac.kr/ee/index.do">Yonsei University</a> in 2016 and 2022, respectively.
        I was a research intern with researchers <a href="https://research.adobe.com/person/zhifei-zhang/">Zhifei Zhang</a> and <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>
        in Creative Intelligence Lab of <a href="https://research.adobe.com/">Adobe Research</a> from June 2021 to November 2021.
        <br>
        <br>
        My current research centers on analyzing local environments of an object or scene of interest without the need of supervision but driven by visual correspondences.
        </p>

        </td>
  </tr>
</table>

<!-- =================== Experience =================== -->
<table width="100%" align="center" style="margin-left:10px" cellspacing="0" cellpadding="0" border="0">
    <tr>
        <th width="20.75%" valign="top" align="center">
        <img src="images/ucb.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">UC Berkeley<br>Postdoctral scholar<br>Mar. 22 - Present</p>
        </th>

        <th width="20.75%" valign="top" align="center">
        <img src="images/icsi.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">ICSI<br>Postdoctoral researcher<br>Mar. 22 - Present</p>
        </th>

        <th width="20.75%" valign="top" align="center">
        <img src="images/adobe.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">Adobe research<br>Research intern<br>May. 21 - Nov. 21</p>
        </th>

        <th width="20.75%" valign="top" align="center">
          <img src="images/yonsei.jpeg" alt="sym" width="54%"></a>
          <p style="line-height:1.3; font-size:12pt">Yonsei university<br>
            Doctor of philosophy<br>Mar. 16 - Feb. 22</p>
        </th>
    </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="8">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
      <li> [10/2022] One paper accepted by <b>BMVC 2022</b></li>
      <li> [09/2022] One paper accepted by <b>NeurIPS 2022</b></li>
      <li> [08/2022] One paper accepted by <b>ECCV workshop 2022</b></li>
      <li> [03/2022] Joined <b>UC Berkeley / ICSI</b> as a postdoctoral scholar</li>
    </ul>
  </td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
<tr><td><sectionheading>&nbsp;&nbsp;Selected Publications</sectionheading>
<ul>
  Please see my <a href="https://scholar.google.com/citations?hl=en&user=3s4zIroAAAAJ">Google Scholar</a> profiles for the full list
</ul>
</td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">


    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/bmvc22.png" alt="sym" width="300" height="240" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="bmvc22">

              <heading>COAT: Correspondence-driven Object Appearance Transfer</heading></a>
              <br>
              <u><b>Sangryul Jeon</b></u>, Zhifei Zhang, Zhe Lin, Scott Cohen, Zhihong Ding, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>BMVC 2022</b>
          </p>

          <div class="paper" id="bmvc22">
          <a href="javascript:toggleblock('bmvc22_abs')">abstract</a> |

          <p align="justify">
              <i id="bmvc22_abs">
                Semantic correspondence is playing an increasingly important role in photorealistic style transfer, especially on objects with prior structural patterns like faces and cars. Unlike traditional methods that are blind to object/non-object regions and spatial correspondence between objects, we propose a new model called correspondence-driven object appearance transfer (COAT), which leverages correspondence to spatially align texture features to content features at multiple scales. Our model does not require extra supervision like semantic segmentation or body parsing and can be adapted to any given generic object category. More importantly, our multi-scale strategy achieves richer texture transfer, while at the same time preserving the spatial structure of objects in the content image. We further propose the correspondence contrastive loss (CCL) with hard negative mining during the training, boosting appearance transfer with improved disentanglement of structural and textural features. Exhaustive experimental evaluation on various objects demonstrates our superior robustness and visual quality as compared to state-of-the-art works.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/neurips22.png" alt="sym" width="300" height="200" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="neurips22">

              <heading>Neural Matching Fields: Implicit Representation of Matching Fields for Visual Correspondence</heading></a>
              <br>
              Sunghwan Hong, Jisu Nam, Seokju Cho, Susung Hong, <u><b>Sangryul Jeon</b></u>, Dongbo Min, Seungryong Kim
              <br>
              <!-- Under review -->
              <b>NeurIPS 2022</b>
          </p>

          <div class="paper" id="neurips22">
          <a href="javascript:toggleblock('neurips22_abs')">abstract</a> |

          <p align="justify">
              <i id="neurips22_abs">
                Existing pipelines of semantic correspondence commonly include extracting high-level semantic features for the invariance against intra-class variations and background clutters. This architecture, however, inevitably results in a low-resolution matching field that additionally requires an ad-hoc interpolation process as a post-processing for converting it into a high-resolution one, certainly limiting the overall performance of matching results. To overcome this, inspired by recent success of implicit neural representation, we present a novel method for semantic correspondence, called neural matching field (NeMF). However, complicacy and high-dimensionality of a 4D matching field are the major hindrances, which we propose a cost embedding network to process a coarse cost volume to use as a guidance for establishing high-precision matching field through the following fully-connected network. Nevertheless, learning a high-dimensional matching field remains challenging mainly due to computational complexity, since a naive exhaustive inference would require querying from all pixels in the 4D space to infer pixel-wise correspondences. To overcome this, we propose adequate training and inference procedures, which in the training phase, we randomly sample matching candidates and in the inference phase, we iteratively performs PatchMatch-based inference and coordinate optimization at test time. With these combined, competitive results are attained on several standard benchmarks for semantic correspondence.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/eccvw22.png" alt="sym" width="300" height="200" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="eccvw22">

              <heading>Unsupervised Scene Sketch to Photo Synthesis</heading></a>
              <br>
              Jiayun Wang, <u><b>Sangryul Jeon</b></u>, Stella X. Yu, Xi Zhang, Himanshu Arora, Yu Lou
              <br>
              <!-- Under review -->
              <b>ECCV Workshop 2022</b> - AIM: Advances in Image Manipulation workshop and challenges
          </p>

          <div class="paper" id="eccvw22">
          <a href="javascript:toggleblock('eccvw22_abs')">abstract</a> |
          <a href="https://arxiv.org/abs/2209.02834">paper</a>

          <p align="justify">
              <i id="eccvw22_abs">
                Sketches make an intuitive and powerful visual expression as they are fast executed freehand drawings. We present a method for synthesizing realistic photos from scene sketches. Without the need for sketch and photo pairs, our framework directly learns from readily available large-scale photo datasets in an unsupervised manner. To this end, we introduce a standardization module that provides pseudo sketch-photo pairs during training by converting photos and sketches to a standardized domain, i.e. the edge map. The reduced domain gap between sketch and photo also allows us to disentangle them into two components: holistic scene structures and low-level visual styles such as color and texture. Taking this advantage, we synthesize a photo-realistic image by combining the structure of a sketch and the visual style of a reference photo. Extensive experimental results on perceptual similarity metrics and human perceptual studies show the proposed method could generate realistic photos with high fidelity from scene sketches and outperform state-of-the-art photo synthesis baselines. We also demonstrate that our framework facilitates a controllable manipulation of photo synthesis by editing strokes of corresponding sketches, delivering more fine-grained details than previous approaches that rely on region-level editing.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/tpami21.png" alt="sym" width="300" height="210" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="tpami21">

              <heading>Pyramidal Semantic Correspondence Networks</heading></a>
              <br>
              <u><b>Sangryul Jeon</b></u>, Seungryong Kim, Dongbo Min, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>TPAMI 2021</b>
          </p>

          <div class="paper" id="tpami21">
          <a href="javascript:toggleblock('tpami21_abs')">abstract</a> |
          <a href="https://ieeexplore.ieee.org/document/9594706">paper</a>

          <p align="justify">
              <i id="tpami21_abs">
                This paper presents a deep architecture, called pyramidal semantic correspondence networks (PSCNet), that estimates locally-varying affine transformation fields across semantically similar images. To deal with large appearance and shape variations that commonly exist among different instances within the same object category, we leverage a pyramidal model where the affine transformation fields are progressively estimated in a coarse-to-fine manner so that the smoothness constraint is naturally imposed. Different from the previous methods which directly estimate global or local deformations, our method first starts to estimate the transformation from an entire image and then progressively increases the degree of freedom of the transformation by dividing coarse cell into finer ones. To this end, we propose two spatial pyramid models by dividing an image in a form of quad-tree rectangles or into multiple semantic elements of an object. Additionally, to overcome the limitation of insufficient training data, a novel weakly-supervised training scheme is introduced that generates progressively evolving supervisions through the spatial pyramid models by leveraging a correspondence consistency across image pairs. Extensive experimental results on various benchmarks including TSS, Proposal Flow-WILLOW, Proposal Flow-PASCAL, Caltech-101, and SPair-71k demonstrate that the proposed method outperforms the lastest methods for dense semantic correspondence.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/neurips21.png" alt="sym" width="300" height="220" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="neurips21">

              <heading>CATs: Cost Aggregation Transformers for Visual Correspondence</heading></a>
              <br>
              Seokju Cho*, Sunghwan Hong*, <u><b>Sangryul Jeon</b></u>, Yunsung Lee, Kwanghoon Sohn, Seungryong Kim
              <br>
              <!-- Under review -->
              <b>NeurIPS 2021</b>

            <br>
            (* indicates equal contribution)
          </p>

          <div class="paper" id="neurips21">
          <a href="javascript:toggleblock('neurips21_abs')">abstract</a> |
          <a href="https://arxiv.org/abs/2106.02520">paper</a>

          <p align="justify">
              <i id="neurips21_abs">
                We propose a novel cost aggregation network, called Cost Aggregation Transformers (CATs), to find dense correspondences between semantically similar images with additional challenges posed by large intra-class appearance and geometric variations. Cost aggregation is a highly important process in matching tasks, which the matching accuracy depends on the quality of its output. Compared to hand-crafted or CNN-based methods addressing the cost aggregation, in that either lacks robustness to severe deformations or inherit the limitation of CNNs that fail to discriminate incorrect matches due to limited receptive fields, CATs explore global consensus among initial correlation map with the help of some architectural designs that allow us to fully leverage self-attention mechanism. Specifically, we include appearance affinity modeling to aid the cost aggregation process in order to disambiguate the noisy initial correlation maps and propose multi-level aggregation to efficiently capture different semantics from hierarchical feature representations. We then combine with swapping self-attention technique and residual connections not only to enforce consistent matching but also to ease the learning process, which we find that these result in an apparent performance boost. We conduct experiments to demonstrate the effectiveness of the proposed model over the latest methods and provide extensive ablation studies.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <table width="100%" align="center" border="0" cellpadding="10">
      <tr>
        <td>
          <sectionheading>&nbsp;&nbsp;Professional Activity</sectionheading>
            <ul>
              <li> Reviewer: CVPR 2020-2022, ICCV 2021, ECCV 2022, NeurIPS 2020-2022, ICLR 2021-2023, ICML 2021-2022, ACCV 2020-2022, WACV 2021-2022, Pattern Recognition, Neurocomputing</li>
            </ul>
        </td>
      </tr>
    </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr><td><br><p align="center"><font size="2">
        Template from this <a href="https://jonbarron.info/">awesome website</a>.
        </font></p></td></tr>
    </table>

    <script xml:space="preserve" language="JavaScript">
    hideallbibs();
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('bmvc22_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('neurips22_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('eccvw22_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('tpami21_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('neurips21_abs');
    </script>

</body>

</html>
