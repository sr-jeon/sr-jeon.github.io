<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }

  /* added by yc */

  padding {
    padding: 50px 0px;
  }

  </style>
  <link rel="icon" type="image/png" href="images/ucb.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Sangryul Jeon</title>
  <meta name="Sangryul Jeon's Homepage" http-equiv="Content-Type" content="Sangryul Jeon's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-64069893-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<!-- <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20"> -->
<table width="960" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Yen-Chi Cheng</font><br> -->
    <pageheading>Sangryul Jeon</pageheading><br>
    <!-- <b>email</b>: charlescheng0117_at_gmail_dot_com -->
    <!-- <b>email</b>: yenchicheng_at_cmu.edu -->
    <!-- <b>email</b>: yenchich_at_andrew.cmu.edu -->
    <!-- <b>email</b>: yenchich_at_cs.cmu.edu -->
    <b>email</b>: srjeon _at_ berkeley.edu
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <!-- <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', '1cm0asg17eccmilnrlah.oge@h',
        [14, 8, 19, 13, 20, 7, 12, 15, 16, 6, 1, 24, 26, 21, 5, 11, 4, 22, 3, 9, 23, 25, 18, 10, 17, 2]);
        // 'emailScramble', 'charlescheng0117@gmail.com',
        // [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]);
    </script> -->
  </p>

  <tr>
    <!-- <td width="32%" valign="top"><a href="images/yenchicheng.png"><img src="images/yenchicheng.png" width="100%" style="border-radius:15px"></a> -->
    <td width="30%" valign="top"><a href="bww_22aug.jpg"><img src="bww_22aug.jpg" width="100%" style="border-radius:15px"></a>
        <p align=center>
          | <a href="https://drive.google.com/file/d/1Hm9-25a-wLDyfM5hiXU7wuXfCO5SIqDT/view?usp=sharing">CV</a> |
          <a href="https://scholar.google.com/citations?user=3s4zIroAAAAJ&hl=en">Google Scholar</a> |
          <a href="https://www.linkedin.com/in/sangryul-jeon-3b697b17a/">LinkedIn</a> |
      </p>
    </td>

    <td width="66%" valign="top" align="justify">
        <p>
        I am currently a postdoctoral scholar in Electrical Engineering and Computer Science
	at <a href="https://eecs.berkeley.edu/">University of California, Berkeley</a>, working in <a href="https://www.icsi.berkeley.edu/icsi/">International Computer Science Institute</a>
	advised by <a href="https://www1.icsi.berkeley.edu/~stellayu/">Prof. Stella X. Yu.</a>
        <br>
        <br>
        I obtained my B.S., and Ph.D. degrees advised by <a href="https://diml.yonsei.ac.kr/">Prof. Kwanghoon Sohn</a>
        in School of Electrical and Electronic Engineering from <a href="https://ee.yonsei.ac.kr/ee/index.do">Yonsei University</a> in 2016 and 2022, respectively.
        I was a research intern with researchers <a href="https://research.adobe.com/person/zhifei-zhang/">Zhifei Zhang</a> and <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>
        in Creative Intelligence Lab of <a href="https://research.adobe.com/">Adobe Research</a> from June 2021 to November 2021.
        <br>
        <br>
        My current research centers on analyzing local environments of an object or scene of interest without the need of supervision but driven by visual correspondences.
        </p>

        </td>
  </tr>
</table>

<!-- =================== Experience =================== -->
<table width="100%" align="center" style="margin-left:10px" cellspacing="0" cellpadding="0" border="0">
    <tr>
        <th width="20.75%" valign="top" align="center">
        <img src="images/ucb.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">UC Berkeley<br>Postdoctral scholar<br>Mar. 22 - Present</p>
        </th>

        <th width="20.75%" valign="top" align="center">
        <img src="images/icsi.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">ICSI<br>Postdoctoral researcher<br>Mar. 22 - Present</p>
        </th>

        <th width="20.75%" valign="top" align="center">
        <img src="images/adobe.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">Adobe research<br>Research intern<br>May. 21 - Nov. 21</p>
        </th>

        <th width="20.75%" valign="top" align="center">
          <img src="images/yonsei.jpeg" alt="sym" width="54%"></a>
          <p style="line-height:1.3; font-size:12pt">Yonsei university<br>
            Doctor of philosophy<br>Mar. 16 - Feb. 22</p>
        </th>
    </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="8">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
      <li> [10/2022] One paper accepted by <b>BMVC 2022</b></li>
      <li> [09/2022] One paper accepted by <b>NeurIPS 2022</b></li>
      <li> [08/2022] One paper accepted by <b>ECCV workshop 2022</b></li>
      <li> [03/2022] Joined <b>UC Berkeley / ICSI</b> as a postdoctoral scholar</li>
    </ul>
  </td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
<tr><td><sectionheading>&nbsp;&nbsp;Selected Publications</sectionheading>
<ul>
  Please see my <a href="https://scholar.google.com/citations?hl=en&user=3s4zIroAAAAJ">Google Scholar</a> profiles for the full list
</ul>
</td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">


    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/bmvc22.png" alt="sym" width="300" height="240" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="bmvc22">

              <heading>COAT: Correspondence-driven Object Appearance Transfer</heading></a>
              <br>
              <u><b>Sangryul Jeon</b></u>, Zhifei Zhang, Zhe Lin, Scott Cohen, Zhihong Ding, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>BMVC 2022</b>
          </p>

          <div class="paper" id="bmvc22">
          <a href="javascript:toggleblock('bmvc22_abs')">abstract</a> |

          <p align="justify">
              <i id="bmvc22_abs">
                Semantic correspondence is playing an increasingly important role in photorealistic style transfer, especially on objects with prior structural patterns like faces and cars. Unlike traditional methods that are blind to object/non-object regions and spatial correspondence between objects, we propose a new model called correspondence-driven object appearance transfer (COAT), which leverages correspondence to spatially align texture features to content features at multiple scales. Our model does not require extra supervision like semantic segmentation or body parsing and can be adapted to any given generic object category. More importantly, our multi-scale strategy achieves richer texture transfer, while at the same time preserving the spatial structure of objects in the content image. We further propose the correspondence contrastive loss (CCL) with hard negative mining during the training, boosting appearance transfer with improved disentanglement of structural and textural features. Exhaustive experimental evaluation on various objects demonstrates our superior robustness and visual quality as compared to state-of-the-art works.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/neurips22.png" alt="sym" width="300" height="200" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="neurips22">

              <heading>Neural Matching Fields: Implicit Representation of Matching Fields for Visual Correspondence</heading></a>
              <br>
              Sunghwan Hong, Jisu Nam, Seokju Cho, Susung Hong, <u><b>Sangryul Jeon</b></u>, Dongbo Min, Seungryong Kim
              <br>
              <!-- Under review -->
              <b>NeurIPS 2022</b>
          </p>

          <div class="paper" id="neurips22">
          <a href="javascript:toggleblock('neurips22_abs')">abstract</a> |

          <p align="justify">
              <i id="neurips22_abs">
                Existing pipelines of semantic correspondence commonly include extracting high-level semantic features for the invariance against intra-class variations and background clutters. This architecture, however, inevitably results in a low-resolution matching field that additionally requires an ad-hoc interpolation process as a post-processing for converting it into a high-resolution one, certainly limiting the overall performance of matching results. To overcome this, inspired by recent success of implicit neural representation, we present a novel method for semantic correspondence, called neural matching field (NeMF). However, complicacy and high-dimensionality of a 4D matching field are the major hindrances, which we propose a cost embedding network to process a coarse cost volume to use as a guidance for establishing high-precision matching field through the following fully-connected network. Nevertheless, learning a high-dimensional matching field remains challenging mainly due to computational complexity, since a naive exhaustive inference would require querying from all pixels in the 4D space to infer pixel-wise correspondences. To overcome this, we propose adequate training and inference procedures, which in the training phase, we randomly sample matching candidates and in the inference phase, we iteratively performs PatchMatch-based inference and coordinate optimization at test time. With these combined, competitive results are attained on several standard benchmarks for semantic correspondence.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/eccvw22.png" alt="sym" width="300" height="200" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="eccvw22">

              <heading>Unsupervised Scene Sketch to Photo Synthesis</heading></a>
              <br>
              Jiayun Wang, <u><b>Sangryul Jeon</b></u>, Stella X. Yu, Xi Zhang, Himanshu Arora, Yu Lou
              <br>
              <!-- Under review -->
              <b>ECCV Workshop 2022</b> - AIM: Advances in Image Manipulation workshop and challenges
          </p>

          <div class="paper" id="eccvw22">
          <a href="javascript:toggleblock('eccvw22_abs')">abstract</a> |
          <a href="https://arxiv.org/abs/2209.02834">paper</a>

          <p align="justify">
              <i id="eccvw22_abs">
                Sketches make an intuitive and powerful visual expression as they are fast executed freehand drawings. We present a method for synthesizing realistic photos from scene sketches. Without the need for sketch and photo pairs, our framework directly learns from readily available large-scale photo datasets in an unsupervised manner. To this end, we introduce a standardization module that provides pseudo sketch-photo pairs during training by converting photos and sketches to a standardized domain, i.e. the edge map. The reduced domain gap between sketch and photo also allows us to disentangle them into two components: holistic scene structures and low-level visual styles such as color and texture. Taking this advantage, we synthesize a photo-realistic image by combining the structure of a sketch and the visual style of a reference photo. Extensive experimental results on perceptual similarity metrics and human perceptual studies show the proposed method could generate realistic photos with high fidelity from scene sketches and outperform state-of-the-art photo synthesis baselines. We also demonstrate that our framework facilitates a controllable manipulation of photo synthesis by editing strokes of corresponding sketches, delivering more fine-grained details than previous approaches that rely on region-level editing.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/tpami21.png" alt="sym" width="300" height="210" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="tpami21">

              <heading>Pyramidal Semantic Correspondence Networks</heading></a>
              <br>
              <u><b>Sangryul Jeon</b></u>, Seungryong Kim, Dongbo Min, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>TPAMI 2021</b>
          </p>

          <div class="paper" id="tpami21">
          <a href="javascript:toggleblock('tpami21_abs')">abstract</a> |
          <a href="https://ieeexplore.ieee.org/document/9594706">paper</a>

          <p align="justify">
              <i id="tpami21_abs">
                This paper presents a deep architecture, called pyramidal semantic correspondence networks (PSCNet), that estimates locally-varying affine transformation fields across semantically similar images. To deal with large appearance and shape variations that commonly exist among different instances within the same object category, we leverage a pyramidal model where the affine transformation fields are progressively estimated in a coarse-to-fine manner so that the smoothness constraint is naturally imposed. Different from the previous methods which directly estimate global or local deformations, our method first starts to estimate the transformation from an entire image and then progressively increases the degree of freedom of the transformation by dividing coarse cell into finer ones. To this end, we propose two spatial pyramid models by dividing an image in a form of quad-tree rectangles or into multiple semantic elements of an object. Additionally, to overcome the limitation of insufficient training data, a novel weakly-supervised training scheme is introduced that generates progressively evolving supervisions through the spatial pyramid models by leveraging a correspondence consistency across image pairs. Extensive experimental results on various benchmarks including TSS, Proposal Flow-WILLOW, Proposal Flow-PASCAL, Caltech-101, and SPair-71k demonstrate that the proposed method outperforms the lastest methods for dense semantic correspondence.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/neurips21.png" alt="sym" width="300" height="220" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="neurips21">

              <heading>CATs: Cost Aggregation Transformers for Visual Correspondence</heading></a>
              <br>
              Seokju Cho*, Sunghwan Hong*, <u><b>Sangryul Jeon</b></u>, Yunsung Lee, Kwanghoon Sohn, Seungryong Kim
              <br>
              <!-- Under review -->
              <b>NeurIPS 2021</b>

            <br>
            (* indicates equal contribution)
          </p>

          <div class="paper" id="neurips21">
          <a href="javascript:toggleblock('neurips21_abs')">abstract</a> |
          <a href="https://arxiv.org/abs/2106.02520">paper</a>

          <p align="justify">
              <i id="neurips21_abs">
                We propose a novel cost aggregation network, called Cost Aggregation Transformers (CATs), to find dense correspondences between semantically similar images with additional challenges posed by large intra-class appearance and geometric variations. Cost aggregation is a highly important process in matching tasks, which the matching accuracy depends on the quality of its output. Compared to hand-crafted or CNN-based methods addressing the cost aggregation, in that either lacks robustness to severe deformations or inherit the limitation of CNNs that fail to discriminate incorrect matches due to limited receptive fields, CATs explore global consensus among initial correlation map with the help of some architectural designs that allow us to fully leverage self-attention mechanism. Specifically, we include appearance affinity modeling to aid the cost aggregation process in order to disambiguate the noisy initial correlation maps and propose multi-level aggregation to efficiently capture different semantics from hierarchical feature representations. We then combine with swapping self-attention technique and residual connections not only to enforce consistent matching but also to ease the learning process, which we find that these result in an apparent performance boost. We conduct experiments to demonstrate the effectiveness of the proposed model over the latest methods and provide extensive ablation studies.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/cvpr21.png" alt="sym" width="300" height="310" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="cvpr21">

              <heading>Mining Better Samples for Contrastive Learning of Temporal Correspondence</heading></a>
              <br>
              <u><b>Sangryul Jeon</b></u>, Dongbo Min, Seungryong Kim, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>CVPR 2021</b>

          </p>

          <div class="paper" id="cvpr21">
          <a href="javascript:toggleblock('cvpr21_abs')">abstract</a> |
          <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Jeon_Mining_Better_Samples_for_Contrastive_Learning_of_Temporal_Correspondence_CVPR_2021_paper.html">paper</a>

          <p align="justify">
              <i id="cvpr21_abs">
                We present a novel framework for contrastive learning of pixel-level representation using only unlabeled video. Without the need of ground-truth annotation, our method is capable of collecting well-defined positive correspondences by measuring their confidences and well-defined negative ones by appropriately adjusting their hardness during training. This allows us to suppress the adverse impact of ambiguous matches and prevent a trivial solution from being yielded by too hard or too easy negative samples. To accomplish this, we incorporate three different criteria that ranges from a pixel-level matching confidence to a video-level one into a bottom-up pipeline, and plan a curriculum that is aware of current representation power for the adaptive hardness of negative samples during training. With the proposed method, state-of-the-art performance is attained over the latest approaches on several video label propagation tasks.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/eccv20.png" alt="sym" width="300" height="182" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="eccv20">

              <heading>Guided Semantic Flow</heading></a>
              <br>
              <u><b>Sangryul Jeon</b></u>, Dongbo Min, Seungryong Kim, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>ECCV 2020</b>

          </p>

          <div class="paper" id="eccv20">
          <a href="javascript:toggleblock('eccv20_abs')">abstract</a> |
          <a href="https://link.springer.com/chapter/10.1007/978-3-030-58604-1_38">paper</a>

          <p align="justify">
              <i id="eccv20_abs">
                Establishing dense semantic correspondences requires dealing with large geometric variations caused by the unconstrained setting of images. To address such severe matching ambiguities, we introduce a novel approach, called guided semantic flow, based on the key insight that sparse yet reliable matches can effectively capture non-rigid geometric variations, and these confident matches can guide adjacent pixels to have similar solution spaces, reducing the matching ambiguities significantly. We realize this idea with learning-based selection of confident matches from an initial set of all pairwise matching scores and their propagation by a new differentiable upsampling layer based on moving least square concept. We take advantage of the guidance from reliable matches to refine the matching hypotheses through Gaussian parametric model in the subsequent matching pipeline. With the proposed method, state-of-the-art performance is attained on several standard benchmarks for semantic correspondence.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/iccv19.png" alt="sym" width="300" height="150" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="iccv19">

              <heading>Joint Learning of Semantic Alignment and Object Landmark Detection</heading></a>
              <br>
              <u><b>Sangryul Jeon</b></u>, Dongbo Min, Seungryong Kim, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>ICCV 2019</b>

          </p>

          <div class="paper" id="iccv19">
          <a href="javascript:toggleblock('iccv19_abs')">abstract</a> |
          <a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Jeon_Joint_Learning_of_Semantic_Alignment_and_Object_Landmark_Detection_ICCV_2019_paper.html">paper</a>

          <p align="justify">
              <i id="iccv19_abs">
                Convolutional neural networks (CNNs) based approaches for semantic alignment and object landmark detection have improved their performance significantly. Current efforts for the two tasks focus on addressing the lack of massive training data through weakly- or unsupervised learning frameworks. In this paper, we present a joint learning approach for obtaining dense correspondences and discovering object landmarks from semantically similar images. Based on the key insight that the two tasks can mutually provide supervisions to each other, our networks accomplish this through a joint loss function that alternatively imposes a consistency constraint between the two tasks, thereby boosting the performance and addressing the lack of training data in a principled manner. To the best of our knowledge, this is the first attempt to address the lack of training data for the two tasks through the joint learning. To further improve the robustness of our framework, we introduce a probabilistic learning formulation that allows only reliable matches to be used in the joint learning process. With the proposed method, state-of-the-art performance is attained on several benchmarks for semantic matching and landmark detection.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/cvpr19.png" alt="sym" width="300" height="150" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="cvpr19">

              <heading>Semantic Attribute Matching Networks</heading></a>
              <br>
              Seungryong Kim, Dongbo Min, Somi Jeong, Sunok Kim, <u><b>Sangryul Jeon</b></u>, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>CVPR 2019</b>

          </p>

          <div class="paper" id="cvpr19">
          <a href="javascript:toggleblock('cvpr19_abs')">abstract</a> |
          <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Semantic_Attribute_Matching_Networks_CVPR_2019_paper.html">paper</a>

          <p align="justify">
              <i id="cvpr19_abs">
                We present semantic attribute matching networks (SAM-Net) for jointly establishing correspondences and transferring attributes across semantically similar images, which intelligently weaves the advantages of the two tasks while overcoming their limitations. SAM-Net accomplishes this through an iterative process of establishing reliable correspondences by reducing the attribute discrepancy between the images and synthesizing attribute transferred images using the learned correspondences. To learn the networks using weak supervisions in the form of image pairs, we present a semantic attribute matching loss based on the matching similarity between an attribute transferred source feature and a warped target feature. With SAM-Net, the state-of-the-art performance is attained on several benchmarks for semantic matching and attribute transfer.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/neurips18.png" alt="sym" width="300" height="215" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="neurips18">

              <heading>Recurrent Transformer Networks for Semantic Correspondence</heading></a>
              <br>
              Seungryong Kim, Stephen Lin, <u><b>Sangryul Jeon</b></u>, Dongbo Min, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>NeurIPS 2018</b> - Spotlight presentation

          </p>

          <div class="paper" id="neurips18">
          <a href="javascript:toggleblock('neurips18_abs')">abstract</a> |
          <a href="https://proceedings.neurips.cc/paper/2018/hash/e4a93f0332b2519177ed55741ea4e5e7-Abstract.html">paper</a>

          <p align="justify">
              <i id="neurips18_abs">
                We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to refine both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classification loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/eccv18.png" alt="sym" width="300" height="132" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="eccv18">

              <heading>PARN: Pyramidal Affine Regression Networks for Dense Semantic Correspondence</heading></a>
              <br>
              <u><b>Sangryul Jeon</b></u>, Seungryong Kim, Dongbo Min, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>ECCV 2018</b>

          </p>

          <div class="paper" id="eccv18">
          <a href="javascript:toggleblock('eccv18_abs')">abstract</a> |
          <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Sangryul_Jeon_PARN_Pyramidal_Affine_ECCV_2018_paper.html">paper</a>

          <p align="justify">
              <i id="eccv18_abs">
                This paper presents a deep architecture for dense semantic correspondence, called pyramidal affine regression networks (PARN), that estimates locally-varying affine transformation fields across images. To deal with intra-class appearance and shape variations that commonly exist among different instances within the same object category, we leverage a pyramidal model where affine transformation fields are progressively estimated in a coarse-to-fine manner so that the smoothness constraint is naturally imposed within deep networks. PARN estimates residual affine transformations at each level and composes them to estimate final affine transformations. Furthermore, to overcome the limitations of insufficient training data for semantic correspondence, we propose a novel weakly-supervised training scheme that generates progressive supervisions by leveraging a correspondence consistency across image pairs. Our method is fully learnable in an end-to-end manner and does not require quantizing infinite continuous affine transformation fields. To the best of our knowledge, it is the first work that attempts to estimate dense affine transformation fields in a coarse-to-fine manner within deep networks. Experimental results demonstrate that PARN outperforms the state-of-the-art methods for dense semantic correspondence on various benchmarks.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <tr>
      <td width="33%" valign="top" align="center">
          <a href="#">
          <img src="images/cvpr17.png" alt="sym" width="300" height="210" style="border-radius:15px">
          </a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="#" id="cvpr17">

              <heading>FCSS: Fully Convolutional Self-Similarity for Dense Semantic Correspondence</heading></a>
              <br>
              Seungryong Kim, Dongbo Min, Bumsub Ham, <u><b>Sangryul Jeon</b></u>, Stephen Lin, Kwanghoon Sohn
              <br>
              <!-- Under review -->
              <b>CVPR 2017</b>

          </p>

          <div class="paper" id="cvpr17">
          <a href="javascript:toggleblock('cvpr17_abs')">abstract</a> |
          <a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Kim_FCSS_Fully_Convolutional_CVPR_2017_paper.html">paper</a>

          <p align="justify">
              <i id="cvpr17_abs">
                We present a descriptor, called fully convolutional self-similarity (FCSS), for dense semantic correspondence. To robustly match points among different instances within the same object class, we formulate FCSS using local self-similarity (LSS) within a fully convolutional network. In contrast to existing CNN-based descriptors, FCSS is inherently insensitive to intra-class appearance variations because of its LSS-based structure, while maintaining the precise localization ability of deep neural networks. The sampling patterns of local structure and the self-similarity measure are jointly learned within the proposed network in an end-to-end and multi-scale manner. As training data for semantic correspondence is rather limited, we propose to leverage object candidate priors provided in existing image datasets and also correspondence consistency between object pairs to enable weakly-supervised learning. Experiments demonstrate that FCSS outperforms conventional handcrafted descriptors and CNN-based descriptors on various benchmarks.
              </i>
          </p>

          </div>
      </td>
    </tr>

    <table width="100%" align="center" border="0" cellpadding="10">
      <tr>
        <td>
          <sectionheading>&nbsp;&nbsp;Professional Activity</sectionheading>
            <ul>
              <li> Reviewer: CVPR 2020-2022, ICCV 2021, ECCV 2022, NeurIPS 2020-2022, ICLR 2021-2023, ICML 2021-2022, ACCV 2020-2022, WACV 2021-2022, Pattern Recognition, Neurocomputing</li>
            </ul>
        </td>
      </tr>
    </table>



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr><td><br><p align="center"><font size="2">
        Template from this <a href="https://jonbarron.info/">awesome website</a>.
        </font></p></td></tr>
    </table>

    <script xml:space="preserve" language="JavaScript">
    hideallbibs();
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('bmvc22_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('neurips22_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('eccvw22_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('tpami21_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('neurips21_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('cvpr21_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('eccv20_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('iccv19_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('cvpr19_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('neurips18_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('eccv18_abs');
    </script>

    <script xml:space="preserve" language="JavaScript">
      hideblock('cvpr17_abs');
    </script>

</body>

</html>
